{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7e5d41a-cfc5-4694-8454-fb91d7dfbd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in ./jupyter_env/lib/python3.12/site-packages (7.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5de2546-846b-4ba7-a66f-4a4936ea4bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pynvml in ./jupyter_env/lib/python3.12/site-packages (12.0.0)\n",
      "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in ./jupyter_env/lib/python3.12/site-packages (from pynvml) (12.570.86)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3319271d-b7c3-4a0f-99e6-9f4c7b44a8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in ./jupyter_env/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./jupyter_env/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./jupyter_env/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./jupyter_env/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./jupyter_env/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in ./jupyter_env/lib/python3.12/site-packages (from matplotlib) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./jupyter_env/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in ./jupyter_env/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./jupyter_env/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./jupyter_env/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./jupyter_env/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05729d72-f791-4793-9cc7-a52e903b8214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in ./jupyter_env/lib/python3.12/site-packages (1.15.2)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in ./jupyter_env/lib/python3.12/site-packages (from scipy) (2.2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75c64a93-42d8-4f24-b9e8-11a00b30b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import psutil\n",
    "import json\n",
    "\n",
    "def run_model(model_name, prompt):\n",
    "    \"\"\"\n",
    "    Exécute un modèle via Docker (Ollama) et retourne la réponse.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Lancer le modèle via Docker\n",
    "    command = [\n",
    "        \"docker\", \"exec\", \"ollama\", \"ollama\", \"run\", model_name, prompt\n",
    "    ]\n",
    "    \n",
    "    # Exécuter la commande\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "    \n",
    "    # Calculer le temps de latence\n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    # Récupérer la réponse du modèle (JSON attendu)\n",
    "    try:\n",
    "        response = json.loads(result.stdout)\n",
    "    except json.JSONDecodeError:\n",
    "        response = {\"output\": result.stdout.strip()}\n",
    "    \n",
    "    return response, latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53bd949-9f75-441e-8946-f86890cbf117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import nvmlInit, nvmlDeviceGetCount, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlDeviceGetUtilizationRates\n",
    "\n",
    "\n",
    "def get_gpu_usage():\n",
    "    \"\"\"\n",
    "    Récupère l'utilisation des ressources GPU via pynvml.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialiser la bibliothèque NVML (NVIDIA Management Library)\n",
    "        nvmlInit()\n",
    "\n",
    "        gpu_stats = []\n",
    "        num_gpus = nvmlDeviceGetCount()  # Nombre de GPU disponibles\n",
    "\n",
    "        # Récupérer les informations de chaque GPU\n",
    "        for i in range(num_gpus):\n",
    "            handle = nvmlDeviceGetHandleByIndex(i)  # Obtenir le handle du GPU\n",
    "            memory_info = nvmlDeviceGetMemoryInfo(handle)  # Obtenir les infos de mémoire\n",
    "            utilization = nvmlDeviceGetUtilizationRates(handle)  # Obtenir l'utilisation du GPU\n",
    "\n",
    "            # Ajouter les informations de chaque GPU dans la liste\n",
    "            gpu_stats.append({\n",
    "                'GPU Index': i,\n",
    "                'GPU Utilization (%)': utilization.gpu,  # Utilisation du GPU en %\n",
    "                'Memory Used (MB)': memory_info.used // 1024,  # Convertir en Mo\n",
    "                'Memory Free (MB)': memory_info.free // 1024,  # Convertir en Mo\n",
    "            })\n",
    "        \n",
    "        return gpu_stats\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la récupération des statistiques GPU avec pynvml: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d62ffce-4d0f-44ca-9b58-1590a39c408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "def monitor_resources():\n",
    "    \"\"\"\n",
    "    Retourne l'utilisation du CPU et de la mémoire.\n",
    "    \"\"\"\n",
    "    cpu_usage = psutil.cpu_percent(interval=1)  # Moyenne sur 1 seconde\n",
    "    memory_usage = psutil.virtual_memory().percent  # Utilisation de la mémoire\n",
    "    return cpu_usage, memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec74313-e113-4d2f-88b3-8aee0f276911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_graphs(latencies, cpu_usages, memory_usages, gpu_utilizations, memory_used, memory_free):\n",
    "    \"\"\"\n",
    "    Trace les courbes pour les différentes valeurs collectées.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Tracer la courbe des latences\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(latencies, label=\"Latence\", color='blue')\n",
    "    plt.xlabel(\"Test\")\n",
    "    plt.ylabel(\"Latence (s)\")\n",
    "    plt.title(\"Latence du modèle\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Tracer la courbe d'utilisation CPU\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(cpu_usages, label=\"CPU Usage\", color='red')\n",
    "    plt.xlabel(\"Test\")\n",
    "    plt.ylabel(\"CPU (%)\")\n",
    "    plt.title(\"Utilisation du CPU\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Tracer la courbe d'utilisation de la mémoire\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(memory_usages, label=\"Memory Usage\", color='green')\n",
    "    plt.xlabel(\"Test\")\n",
    "    plt.ylabel(\"Memory (%)\")\n",
    "    plt.title(\"Utilisation de la mémoire\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Tracer les courbes des ressources GPU\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(gpu_utilizations, label=\"GPU Utilization\", color='purple')\n",
    "    plt.plot(memory_used, label=\"Memory Used\", color='orange')\n",
    "    plt.plot(memory_free, label=\"Memory Free\", color='brown')\n",
    "    plt.xlabel(\"Test\")\n",
    "    plt.ylabel(\"GPU Resources (MB / %)\")\n",
    "    plt.title(\"Ressources GPU\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Afficher les graphes\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "904a7425-5abc-4a33-aece-9d40cf69c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import trim_mean\n",
    "\n",
    "def load_questions_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Charge les questions à partir d'un fichier texte.\n",
    "    Chaque ligne du fichier doit contenir une question.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            questions = file.readlines()\n",
    "        # Nettoyer les espaces blancs et les retours à la ligne\n",
    "        questions = [question.strip() for question in questions]\n",
    "        return questions\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la lecture du fichier {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def test_models(prompts, models, trim_percent=0.05):\n",
    "    for model_name in models:\n",
    "        # Initialisation des listes spécifiques à chaque modèle\n",
    "        latencies = []\n",
    "        cpu_usages = []\n",
    "        memory_usages = []\n",
    "        gpu_utilizations = []\n",
    "        memory_used = []\n",
    "        memory_free = []\n",
    "\n",
    "        for prompt in prompts:\n",
    "            response, latency = run_model(model_name, prompt)\n",
    "            cpu, memory = monitor_resources()\n",
    "\n",
    "            latencies.append(latency)\n",
    "            cpu_usages.append(cpu)\n",
    "            memory_usages.append(memory)\n",
    "\n",
    "            # GPU\n",
    "            gpu_usage = get_gpu_usage()\n",
    "            if gpu_usage:\n",
    "                for gpu in gpu_usage:\n",
    "                    gpu_utilizations.append(gpu['GPU Utilization (%)'])\n",
    "                    memory_used.append(gpu['Memory Used (MB)'])\n",
    "                    memory_free.append(gpu['Memory Free (MB)'])\n",
    "\n",
    "        num_tests = len(latencies)\n",
    "\n",
    "        # Moyennes simples\n",
    "        avg_latency = sum(latencies) / num_tests\n",
    "        avg_cpu = sum(cpu_usages) / num_tests\n",
    "        avg_memory = sum(memory_usages) / num_tests\n",
    "\n",
    "        # Trimmed mean\n",
    "        trimmed_latency = trim_mean(latencies, proportiontocut=trim_percent)\n",
    "        trimmed_cpu = trim_mean(cpu_usages, proportiontocut=trim_percent)\n",
    "        trimmed_memory = trim_mean(memory_usages, proportiontocut=trim_percent)\n",
    "        trimmed_gpu = trim_mean(gpu_utilizations, proportiontocut=trim_percent) if gpu_utilizations else 0\n",
    "        trimmed_mem_used = trim_mean(memory_used, proportiontocut=trim_percent) if memory_used else 0\n",
    "        trimmed_mem_free = trim_mean(memory_free, proportiontocut=trim_percent) if memory_free else 0\n",
    "\n",
    "        # Throughput = total requests / total time\n",
    "        total_time = sum(latencies)\n",
    "        throughput = num_tests / total_time if total_time > 0 else 0\n",
    "\n",
    "        # Résultats\n",
    "        print(f\"\\n--- Performance Results for {model_name} ---\")\n",
    "        print(f\"Simple Mean Latency: {avg_latency:.4f} sec\")\n",
    "        print(f\"Simple Mean CPU Usage: {avg_cpu:.2f}%\")\n",
    "        print(f\"Simple Mean Memory Usage: {avg_memory:.2f}%\")\n",
    "\n",
    "        print(f\"\\nTrimmed Mean Results ({int(trim_percent*100)}% trimming):\")\n",
    "        print(f\"Latency: {trimmed_latency:.4f} sec\")\n",
    "        print(f\"CPU Usage: {trimmed_cpu:.2f}%\")\n",
    "        print(f\"Memory Usage: {trimmed_memory:.2f}%\")\n",
    "        print(f\"GPU Utilization: {trimmed_gpu:.2f}%\")\n",
    "        print(f\"GPU Memory Used: {trimmed_mem_used:.2f} MB\")\n",
    "        print(f\"GPU Memory Free: {trimmed_mem_free:.2f} MB\")\n",
    "\n",
    "        print(f\"\\nThroughput: {throughput:.2f} requests per second\")\n",
    "\n",
    "        # Optionnel : tracer les courbes pour ce modèle uniquement\n",
    "        plot_graphs(latencies, cpu_usages, memory_usages, gpu_utilizations, memory_used, memory_free)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bbc99a-e210-4d96-a95a-bcb7ac19cb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performance Results for llama3 ---\n",
      "Simple Mean Latency: 7.8861 sec\n",
      "Simple Mean CPU Usage: 4.01%\n",
      "Simple Mean Memory Usage: 15.33%\n",
      "\n",
      "Trimmed Mean Results (5% trimming):\n",
      "Latency: 7.6899 sec\n",
      "CPU Usage: 1.76%\n",
      "Memory Usage: 15.31%\n",
      "GPU Utilization: 0.00%\n",
      "GPU Memory Used: 11813248.00 MB\n",
      "GPU Memory Free: 3915392.00 MB\n",
      "\n",
      "Throughput: 0.13 requests per second\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# def process_questions(input_file):\n",
    "#     \"\"\"\n",
    "#     Traite une liste de questions à partir d'un fichier et exécute chaque question sur les modèles.\n",
    "#     \"\"\"\n",
    "#     with open(input_file, 'r') as file:\n",
    "#         questions = file.readlines()\n",
    "    \n",
    "#     results = []\n",
    "    \n",
    "#     for question in questions:\n",
    "#         question = question.strip()  # Enlever les espaces superflus\n",
    "        \n",
    "#         # Tester Llama3\n",
    "#         response, latency = run_model(\"llama3\", question)\n",
    "#         cpu, memory = monitor_resources()\n",
    "#         results.append({\n",
    "#             \"model\": \"Llama3\",\n",
    "#             \"prompt\": question,\n",
    "#             \"response\": response['output'],\n",
    "#             \"latency\": latency,\n",
    "#             \"cpu_usage\": cpu,\n",
    "#             \"memory_usage\": memory\n",
    "#         })\n",
    "\n",
    "#         # Tester DeepSeek-R1\n",
    "#         response, latency = run_model(\"deepseek-r1\", question)\n",
    "#         cpu, memory = monitor_resources()\n",
    "#         results.append({\n",
    "#             \"model\": \"DeepSeek-R1\",\n",
    "#             \"prompt\": question,\n",
    "#             \"response\": response['output'],\n",
    "#             \"latency\": latency,\n",
    "#             \"cpu_usage\": cpu,\n",
    "#             \"memory_usage\": memory\n",
    "#         })\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# def save_results_to_file(results, output_file):\n",
    "#     \"\"\"\n",
    "#     Sauvegarde les résultats dans un fichier JSON.\n",
    "#     \"\"\"\n",
    "#     with open(output_file, 'w') as file:\n",
    "#         json.dump(results, file, indent=4)\n",
    "        \n",
    "# output_file = 'model_results.json'  # Fichier pour sauvegarder les résultats\n",
    "questions_file = 'questions.txt'\n",
    "\n",
    "# results = process_questions(questions_file)\n",
    "# save_results_to_file(results, output_file)\n",
    "prompts = load_questions_from_file(questions_file)  # Charger les questions depuis le fichier\n",
    "\n",
    "models = [\"llama3\", \"deepseek-r1\"]\n",
    "\n",
    "test_models(prompts, models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d261a9-6f56-4dec-b7a8-472a06438924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
